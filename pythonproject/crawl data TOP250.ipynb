{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd1b17a",
   "metadata": {},
   "source": [
    "# 爬取数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a285f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from openpyxl import Workbook, load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f08541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取第0页,请稍等...\n",
      "正在爬取第1页,请稍等...\n",
      "正在爬取第2页,请稍等...\n",
      "正在爬取第3页,请稍等...\n",
      "正在爬取第4页,请稍等...\n",
      "正在爬取第5页,请稍等...\n",
      "正在爬取第6页,请稍等...\n",
      "正在爬取第7页,请稍等...\n",
      "正在爬取第8页,请稍等...\n",
      "正在爬取第9页,请稍等...\n",
      "正在爬取第10页,请稍等...\n"
     ]
    }
   ],
   "source": [
    "#循环构建榜单每一页的url\n",
    "for i in range(11):\n",
    "        print(f'正在爬取第{i}页,请稍等...')\n",
    "        url = 'https://movie.douban.com/top250?start={}&filter='.format(i * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ce2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取每页25部电影链接\n",
    "def getonepagelist(url, headers):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        lsts = soup.find_all(attrs={'class': 'hd'})\n",
    "        for lst in lsts:\n",
    "            href = lst.a['href']\n",
    "            time.sleep(0.5)\n",
    "            getfilminfo(href, headers)\n",
    "    except Exception as e:\n",
    "        print('getonepagelist error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25ece66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取每部电影具体信息\n",
    "def getfilminfo(url, headers):\n",
    "    filminfo = []\n",
    "    r = requests.get(url, headers=headers, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # 片名\n",
    "    name = soup.find(attrs={'property': 'v:itemreviewed'}).text.split(' ')[0]\n",
    "    # 上映年份\n",
    "    year = soup.find(attrs={'class': 'year'}).text.replace('(','').replace(')','')\n",
    "    # 评分\n",
    "    score = soup.find(attrs={'property': 'v:average'}).text\n",
    "    # 评价人数\n",
    "    votes = soup.find(attrs={'property': 'v:votes'}).text\n",
    "    \n",
    "    infos = soup.find(attrs={'id': 'info'}).text.split('\\n')\n",
    "    # 提取所需信息\n",
    "    director = scriptwriter = actor = filmtype = area = language = ''\n",
    "    for info in infos:\n",
    "        if '导演' in info:\n",
    "            director = info.split(': ')[1]\n",
    "        elif '编剧' in info:\n",
    "            scriptwriter = info.split(': ')[1]\n",
    "        elif '主演' in info:\n",
    "            actor = info.split(': ')[1]\n",
    "        elif '类型' in info:\n",
    "            filmtype = info.split(': ')[1]\n",
    "        elif '制片国家/地区' in info:\n",
    "            area = info.split(': ')[1]\n",
    "        elif '语言' in info:\n",
    "            language = info.split(': ')[1]\n",
    "    \n",
    "    if '.' in area:\n",
    "        area = area.split(' / ')[0]\n",
    "        language = language.split(' / ')[0]\n",
    "    else:\n",
    "        area = area.split(' / ')[0]\n",
    "        language = language.split(' / ')[0]\n",
    "\n",
    "    if '大陆' in area or '香港' in area or '台湾' in area:\n",
    "        area = '中国'\n",
    "    if '戛纳' in area:\n",
    "        area = '法国'\n",
    "\n",
    "    # 时长\n",
    "    times0 = soup.find(attrs={'property': 'v:runtime'}).text\n",
    "    times = re.findall('\\d+', times0)[0]\n",
    "    \n",
    "    filminfo.extend([name, year, score, votes, director, scriptwriter, actor, filmtype, area, language, times])\n",
    "    filepath = 'TOP250.xlsx'\n",
    "    insert2excel(filepath, filminfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d4948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用openpyxl将数据保存到Excel中\n",
    "def insert2excel(filepath, allinfo):\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            tableTitle = ['片名', '上映年份', '评分', '评价人数', '导演', '编剧', '主演', '类型', '国家/地区', '语言', '时长(分钟)']\n",
    "            wb = Workbook()\n",
    "            ws = wb.active\n",
    "            ws.title = 'sheet1'\n",
    "            ws.append(tableTitle)\n",
    "            wb.save(filepath)\n",
    "            time.sleep(3)\n",
    "\n",
    "        wb = load_workbook(filepath)\n",
    "        ws = wb.active\n",
    "        ws.title = 'sheet1'\n",
    "        ws.append(allinfo)\n",
    "        wb.save(filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"插入数据到Excel时出错:\", e)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46d03b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据成功插入到Excel文件中\n"
     ]
    }
   ],
   "source": [
    "# 示例调用\n",
    "filminfo = ['电影名', '2024', '9.0', '100000', '导演名', '编剧名', '主演名', '类型', '中国', '中文', '120']\n",
    "filepath = 'TOP250.xlsx'\n",
    "success = insert2excel(filepath, filminfo)\n",
    "if success:\n",
    "    print(\"数据成功插入到Excel文件中\")\n",
    "else:\n",
    "    print(\"数据插入到Excel文件时出错\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b5ea0",
   "metadata": {},
   "source": [
    "# 爬虫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36916b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#汇总运行，爬取数据\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from fake_useragent import UserAgent\n",
    "# os：用于操作系统相关的功能，如检查文件是否存在。\n",
    "# re：用于正则表达式操作，提取文本中的特定模式。\n",
    "# time：用于时间相关的功能，如休眠一段时间。\n",
    "# requests：用于发送HTTP请求。\n",
    "# BeautifulSoup：用于解析HTML和XML文档。\n",
    "# openpyxl：用于操作Excel文件。\n",
    "# fake_useragent：用于生成随机的User-Agent，以模拟不同的浏览器请求。\n",
    "\n",
    "# 获取电影列表页数据\n",
    "def getonepagelist(url, headers):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()    #如果请求失败，抛出异常\n",
    "        r.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')  #解析页面HTML。\n",
    "        lsts = soup.find_all(attrs={'class': 'hd'}) #找到所有具有class属性值为'hd'的标签。\n",
    "        hrefs = []         #用于存储电影详情页的链接。\n",
    "        for lst in lsts:\n",
    "            href = lst.a['href']\n",
    "            hrefs.append(href)\n",
    "            time.sleep(0.5)   #每获取一个链接后休眠0.5秒，防止请求过快。\n",
    "        return hrefs       \n",
    "    except Exception as e:\n",
    "        print('getonepagelist error:', e)\n",
    "        return []\n",
    "\n",
    "# 获取每部电影的具体信息\n",
    "def getfilminfo(url, headers):\n",
    "    filminfo = []          # 用于存储电影信息的列表。\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # 片名        .text：提取标签中的文本\n",
    "        name = soup.find(attrs={'property': 'v:itemreviewed'}).text.split(' ')[0]\n",
    "        # 上映年份\n",
    "        year = soup.find(attrs={'class': 'year'}).text.replace('(','').replace(')','')\n",
    "        # 评分\n",
    "        score = soup.find(attrs={'property': 'v:average'}).text\n",
    "        # 评价人数\n",
    "        votes = soup.find(attrs={'property': 'v:votes'}).text\n",
    "        \n",
    "        infos = soup.find(attrs={'id': 'info'}).text.split('\\n')\n",
    "        # 提取所需信息\n",
    "        director = scriptwriter = actor = filmtype = area = language = ''\n",
    "        for info in infos:                            #初始化为空字符串，用于存储对应的信息。\n",
    "            if '导演' in info:\n",
    "                director = info.split(': ')[1]\n",
    "            elif '编剧' in info:\n",
    "                scriptwriter = info.split(': ')[1]\n",
    "            elif '主演' in info:\n",
    "                actor = info.split(': ')[1]\n",
    "            elif '类型' in info:\n",
    "                filmtype = info.split(': ')[1]\n",
    "            elif '制片国家/地区' in info:\n",
    "                area = info.split(': ')[1]\n",
    "            elif '语言' in info:\n",
    "                language = info.split(': ')[1]\n",
    "        \n",
    "        if '.' in area:        #如果国家/地区包含'.'，则进行特殊处理。\n",
    "            area = area.split(' / ')[0]\n",
    "            language = language.split(' / ')[0]\n",
    "        else:\n",
    "            area = area.split(' / ')[0]\n",
    "            language = language.split(' / ')[0]\n",
    "\n",
    "        if '大陆' in area or '香港' in area or '台湾' in area:\n",
    "            area = '中国'\n",
    "        if '戛纳' in area:\n",
    "            area = '法国'\n",
    "\n",
    "        # 时长\n",
    "        times0 = soup.find(attrs={'property': 'v:runtime'}).text\n",
    "        times = re.findall('\\d+', times0)[0]\n",
    "        \n",
    "        filminfo.extend([name, year, score, votes, director, scriptwriter, actor, filmtype, area, language, times])\n",
    "    except Exception as e:                         #将所有信息添加到filminfo列表中\n",
    "        print('getfilminfo error:', e)\n",
    "    \n",
    "    return filminfo\n",
    "\n",
    "# 将数据插入到Excel中\n",
    "def insert2excel(filepath, allinfo):\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            tableTitle = ['片名', '上映年份', '评分', '评价人数', '导演', '编剧', '主演', '类型', '国家/地区', '语言', '时长(分钟)']\n",
    "            wb = Workbook()         #创建一个新的Excel工作簿\n",
    "            ws = wb.active\n",
    "            ws.title = 'sheet1'\n",
    "            ws.append(tableTitle)\n",
    "            wb.save(filepath)\n",
    "            time.sleep(3)\n",
    "\n",
    "        wb = load_workbook(filepath)\n",
    "        ws = wb.active\n",
    "        ws.title = 'sheet1'\n",
    "        for info in allinfo:\n",
    "            ws.append(info)\n",
    "        wb.save(filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"插入数据到Excel时出错:\", e)\n",
    "        return False\n",
    "\n",
    "# 主程序\n",
    "def main():\n",
    "    base_url = 'https://movie.douban.com/top250'   #豆瓣电影Top 250页面的基本URL。\n",
    "    headers = {'User-Agent': UserAgent().random}   #HTTP请求头，包含随机的User-Agent\n",
    "    all_movie_info = []\n",
    "\n",
    "    for start in range(0, 250, 25):  # 遍历所有页面，每页25个电影\n",
    "        page_url = f'{base_url}?start={start}'\n",
    "        movie_urls = getonepagelist(page_url, headers)\n",
    "        for movie_url in movie_urls:\n",
    "            movie_info = getfilminfo(movie_url, headers)\n",
    "            if movie_info:\n",
    "                all_movie_info.append(movie_info)\n",
    "            time.sleep(1)  # 避免请求过快\n",
    "\n",
    "    filepath = 'TOP250.xlsx'\n",
    "    if insert2excel(filepath, all_movie_info):\n",
    "        print(\"所有数据已成功插入到Excel文件中\")\n",
    "    else:\n",
    "        print(\"插入数据到Excel文件时出错\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4d16d",
   "metadata": {},
   "source": [
    "# 什么是User-Agent？\n",
    "User-Agent 是一个 HTTP 头字段，包含了关于请求发起方（即用户使用的浏览器、操作系统和设备类型等信息）的详细描述。每次浏览器发送请求时，都会包含这个字段，以便服务器了解请求来自哪个设备、浏览器和操作系统。\n",
    "\n",
    "## 可以使用fake_useragent库生成随机的User-Agent\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "#### 创建一个UserAgent对象\n",
    "ua = UserAgent()\n",
    "\n",
    "#### 获取一个随机的User-Agent字符串\n",
    "random_user_agent = ua.random\n",
    "\n",
    "#### 将随机的User-Agent添加到请求头\n",
    "headers = {'User-Agent': random_user_agent}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38dba7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch robots.txt: 418 Client Error:  for url: https://movie.douban.com/top250/robots.txt\n",
      "* is not allowed to fetch https://movie.douban.com/top250\n"
     ]
    }
   ],
   "source": [
    "# #了解哪些可以抓取\n",
    "# import requests\n",
    "# from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# def fetch_robots_txt(url):\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=10)\n",
    "#         response.raise_for_status()\n",
    "#         return response.text\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"Failed to fetch robots.txt: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def can_fetch(base_url, user_agent):\n",
    "#     robots_url = base_url + \"/robots.txt\"\n",
    "#     robots_txt = fetch_robots_txt(robots_url)\n",
    "#     if robots_txt:\n",
    "#         parser = RobotFileParser()\n",
    "#         parser.parse(robots_txt.splitlines())\n",
    "#         return parser.can_fetch(user_agent, base_url)\n",
    "#     return False\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     base_url = \"https://movie.douban.com\"\n",
    "#     user_agent = \"*\"\n",
    "#     check_url = base_url + \"/top250\"\n",
    "#     if can_fetch(check_url, user_agent):\n",
    "#         print(f\"{user_agent} is allowed to fetch {check_url}\")\n",
    "#     else:\n",
    "#         print(f\"{user_agent} is not allowed to fetch {check_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a82f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #可视化\n",
    "# import pandas as pd\n",
    "# from pyecharts import options as opts\n",
    "# from pyecharts.charts import Bar\n",
    "\n",
    "# data = pd.read_excel('TOP250.xlsx')\n",
    "\n",
    "# # 提取年份数据\n",
    "# year = data['上映年份'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f670c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
